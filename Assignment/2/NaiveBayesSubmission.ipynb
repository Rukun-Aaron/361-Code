{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Explain and motivate the chosen representation & data preprocessing\n",
    "\n",
    "       Bag of words is a simple method of representing the frequency of words in textual inputs. It keeps a record of all the unique words and their respective frequencies. This data is pivotal to our approach in document classification.  Despite its simplicity, Bag of words enables our model to capture important words and their associated frequencies that is useful for classification. \n",
    "\n",
    "       As for data preprocessing, I used stemming to reduce words to the base form and remove any \n",
    "       <!--I chose to tackle to the problem of document classification based on textual content of scientific papers by using bag of words. I is a simple representation of the frequecy of words in a given class.  -->\n",
    " \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Naive Bayes implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from decimal import Decimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Methods for Naive Bayes implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gets the set of all words in the dataframe\n",
    "# def get_vocab(df):\n",
    "\n",
    "#     vocab = {word for para in df['abstract'] for word in para.split() }\n",
    "#     return vocab\n",
    "\n",
    "# # Returns the prior probabilities of all the target classes A, B, E and V\n",
    "# def get_prior_probability(df):\n",
    "#     prior_probability = df['target'].value_counts(normalize=True).to_dict()\n",
    "#     for key, value in prior_probability.items():\n",
    "#         prior_probability[key] = Decimal(str(value))\n",
    "#     return prior_probability \n",
    "\n",
    "# # Returns a dictionary containing all the words \n",
    "# def get_conditional_probability(df):\n",
    "#     conditional_probability = dict()\n",
    "    \n",
    "#     for row in df.itertuples(index = 0): \n",
    "     \n",
    "#         line = row.abstract\n",
    "#         target = row.target\n",
    "#         words = [word for word in line.split()]\n",
    "\n",
    "#         for word in words :\n",
    "            \n",
    "#             if word in conditional_probability.keys():\n",
    "#                 if target in conditional_probability[word].keys():\n",
    "#                     conditional_probability[word][target] += 1\n",
    "#                 else:\n",
    "#                     conditional_probability[word][target] = 1\n",
    "#             else:\n",
    "#                 conditional_probability[word] = {\"B\": Decimal(0), \"E\":Decimal(0), \"A\":Decimal(0), \"V\":Decimal(0)}\n",
    "#                 conditional_probability[word][target] += 1\n",
    "#     return conditional_probability\n",
    "\n",
    "# ## Convert conditional probabilities dictionary to a dataframe\n",
    "# def get_conditional_probability_df(conditional_probability):\n",
    "\n",
    "#     data = []\n",
    "#     for word, targets in conditional_probability.items():\n",
    "        \n",
    "#         row = {'word': word}\n",
    "        \n",
    "#         for i, (target, count) in enumerate(targets.items()):\n",
    "#             row [target] = count\n",
    "#         data.append(row)\n",
    "#     word_orrurance_table = pd.DataFrame(data)\n",
    "#     word_orrurance_table.fillna(0, inplace=True)\n",
    "#     return word_orrurance_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(df):\n",
    "#     vocab = get_vocab(df)\n",
    "#     prior_probability = get_prior_probability(df)\n",
    "#     conditional_probability_dict = get_conditional_probability(df)\n",
    "#     conditional_probability_df = get_conditional_probability_df(conditional_probability_dict)\n",
    "#     return vocab, prior_probability, conditional_probability_dict,conditional_probability_df\n",
    "\n",
    "# def Naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df,testing):\n",
    "#     predictions = []\n",
    "#     row_nm = 0\n",
    "#     for row in testing.itertuples(index = 0):\n",
    "#         row_nm += 1\n",
    "#         line = row.abstract\n",
    "#         prediction = predict(vocab,prior_probability,conditional_probability_dict,conditional_probability_df,line)\n",
    "#         predictions.append(prediction)\n",
    "#     return predictions\n",
    "# def predict(vocab,prior_probability,conditional_probability_dict,conditional_probability_df,line):\n",
    "#     words = line.split()\n",
    "    \n",
    "#     prob_B = prior_probability['B']\n",
    "#     prob_E = prior_probability['E']\n",
    "#     prob_A = prior_probability['A']\n",
    "#     prob_V = prior_probability['V']\n",
    "\n",
    "#     for word in words:\n",
    "#         if word in vocab:\n",
    "#             prob_B *= conditional_probability_dict[word]['B'] \n",
    "#             prob_A *= conditional_probability_dict[word]['A'] \n",
    "#             prob_V *= conditional_probability_dict[word]['V'] \n",
    "#             prob_E *= conditional_probability_dict[word]['E'] \n",
    "    \n",
    "#     max_prob = max(prob_B, prob_A, prob_V, prob_E)\n",
    "#     if max_prob == prob_E:\n",
    "#         return 'E'\n",
    "#     elif max_prob == prob_B:\n",
    "#         return 'B'\n",
    "#     elif max_prob == prob_A:\n",
    "#         return 'A'\n",
    "#     elif max_prob == prob_V:\n",
    "#         return 'V'\n",
    "#     else:\n",
    "#         return 'E'\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training And Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df = pd.read_csv(\"trg.csv\")\n",
    "# df = df.rename({'class':'target'},axis=1)\n",
    "# training, testing = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# vocab, prior_probability, conditional_probability_dict,conditional_probability_df = train(training)\n",
    "# prediction_row = Naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df, testing)\n",
    "# print(prediction_row)\n",
    "# validation_acc = (testing['target'] == prediction_row).mean()\n",
    "# print(f\"Training acc: {validation_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The standard Naive Bayes impletation yields a validation accuracy of 81.58."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Naive Bayes Algorithm\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neccessay Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from decimal import Decimal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining stop words and applying stemming to the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "stemmed_stop_words = [porter.stem(word) for word in stopWords]\n",
    "stopWords.update(stemmed_stop_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Methods for Improved Naive Bayes implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Improved_get_vocab(df):\n",
    "    # stopWords = set(stopwords.words('english'))\n",
    "    vocab = {word for para in df['abstract'] for word in para.split() if word not in stopWords}\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    vocab = {porter.stem(word) for para in df['abstract'] for word in para.split() if word not in stopWords}\n",
    "    return vocab\n",
    "\n",
    "def Improved_get_prior_probability(df):\n",
    "    prior_probability = df['target'].value_counts(normalize=True).to_dict()\n",
    "    \n",
    "    for key, value in prior_probability.items():\n",
    "        prior_probability[key] = Decimal(str(value))\n",
    "    return prior_probability \n",
    "\n",
    "def Improved_get_conditional_probability(df,vocab):\n",
    "    conditional_probability = dict()\n",
    "    for row in df.itertuples(index = 0): \n",
    "        line = row.abstract\n",
    "        target = row.target\n",
    "        porter = PorterStemmer()\n",
    "        words = [porter.stem(word) for word in line.split() if word not in stopWords]\n",
    "\n",
    "        for word in words :\n",
    "                        \n",
    "            if word in conditional_probability.keys():\n",
    "                if target in conditional_probability[word].keys():\n",
    "                    conditional_probability[word][target] += 1\n",
    "                else:\n",
    "                    conditional_probability[word][target] = 1\n",
    "            else:\n",
    "                conditional_probability[word] = {\"B\": Decimal(0), \"E\":Decimal(0), \"A\":Decimal(0), \"V\":Decimal(0)}\n",
    "                conditional_probability[word][target] += 1\n",
    "    return conditional_probability\n",
    "\n",
    "def Improved_get_conditional_probability_df(conditional_probability):\n",
    "\n",
    "    data = []\n",
    "    for word, targets in conditional_probability.items():\n",
    "        \n",
    "        row = {'word': word}\n",
    "        \n",
    "        for i, (target, count) in enumerate(targets.items()):\n",
    "            row [target] = count\n",
    "        data.append(row)\n",
    "    word_orrurance_table = pd.DataFrame(data)\n",
    "    word_orrurance_table.fillna(0, inplace=True)\n",
    "    return word_orrurance_table\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Improved Navie Bayes Algorithm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The improvments are of a result of Laplace Smoothing, the use of stop words and stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def Improved_train(df):\n",
    "    \n",
    "    vocab = Improved_get_vocab(df)\n",
    "    prior_probability = Improved_get_prior_probability(df)\n",
    "    conditional_probability_dict = Improved_get_conditional_probability(df,vocab)\n",
    "    conditional_probability_df = Improved_get_conditional_probability_df(conditional_probability_dict)\n",
    "    return vocab, prior_probability, conditional_probability_dict,conditional_probability_df\n",
    "\n",
    "def Improved_Naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df,testing):\n",
    "    print(\"Predicting....\") \n",
    "    predictions = []\n",
    "    row_nm = 0\n",
    "    for row in testing.itertuples(index = 0):\n",
    "        row_nm += 1\n",
    "        line = row.abstract\n",
    "        prediction = Improved_predict(vocab,prior_probability,conditional_probability_dict,conditional_probability_df,line)\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def Improved_predict(vocab,prior_probability,conditional_probability_dict,conditional_probability_df,line):\n",
    "    words = line.split()\n",
    "    porter = PorterStemmer()\n",
    "    words = [ porter.stem(word) for word in words if word not in stopWords]\n",
    "    prob_B = prior_probability['B'].ln()\n",
    "    prob_E = prior_probability['E'].ln()\n",
    "    prob_A = prior_probability['A'].ln()\n",
    "    prob_V = prior_probability['V'].ln()\n",
    "\n",
    "    total_B = conditional_probability_df['B'].sum()\n",
    "    total_E = conditional_probability_df['E'].sum()\n",
    "    total_A = conditional_probability_df['A'].sum()\n",
    "    total_V = conditional_probability_df['V'].sum()\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            prob_B += (conditional_probability_dict[word]['B'] + 1).ln() - (Decimal(total_B +  len(vocab))).ln()\n",
    "            prob_A += (conditional_probability_dict[word]['A'] + 1).ln() - (Decimal(total_A + len(vocab))).ln()\n",
    "            prob_V += (conditional_probability_dict[word]['V'] + 1).ln() - (Decimal(total_V + len(vocab))).ln()\n",
    "            prob_E += (conditional_probability_dict[word]['E'] + 1).ln() - (Decimal(total_E + len(vocab))).ln()\n",
    "    max_prob = max(prob_B, prob_A, prob_V, prob_E)\n",
    "    if max_prob == prob_E:\n",
    "        return 'E'\n",
    "    elif max_prob == prob_B:\n",
    "        return 'B'\n",
    "    elif max_prob == prob_A:\n",
    "        return 'A'\n",
    "    elif max_prob == prob_V:\n",
    "        return 'V'\n",
    "    else:\n",
    "        return 'E'\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting....\n",
      "['A', 'V', 'A', 'V', 'V', 'V', 'B', 'V', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'V', 'B', 'E', 'E', 'E', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'A', 'V', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'E', 'B', 'A', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'V', 'B', 'B', 'V', 'V', 'E', 'A', 'A', 'V', 'B', 'B', 'V', 'B', 'E', 'B', 'E', 'V', 'B', 'B', 'V', 'V', 'E', 'B', 'B', 'A', 'A', 'A', 'B', 'E', 'E', 'V', 'V', 'B', 'A', 'A', 'B', 'A', 'E', 'V', 'B', 'E', 'B', 'A', 'V', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'A', 'A', 'V', 'E', 'B', 'V', 'V', 'A', 'E', 'A', 'V', 'B', 'V', 'E', 'B', 'B', 'B', 'V', 'V', 'B', 'V', 'A', 'B', 'B', 'A', 'A', 'V', 'A', 'B', 'B', 'A', 'E', 'A', 'V', 'V', 'V', 'V', 'V', 'V', 'B', 'E', 'V', 'B', 'E', 'E', 'E', 'A', 'E', 'E', 'A', 'A', 'V', 'E', 'B', 'E', 'E', 'B', 'A', 'B', 'E', 'A', 'A', 'A', 'V', 'V', 'B', 'V', 'A', 'B', 'A', 'V', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'V', 'B', 'E', 'V', 'A', 'B', 'A', 'A', 'E', 'B', 'B', 'B', 'V', 'A', 'E', 'V', 'A', 'V', 'B', 'B', 'B', 'E', 'B', 'A', 'A', 'V', 'B', 'E', 'A', 'A', 'V', 'A', 'E', 'A', 'E', 'B', 'B', 'E', 'A', 'B', 'E', 'V', 'E', 'B', 'A', 'V', 'E', 'B', 'A', 'V', 'V', 'A', 'V', 'A', 'B', 'V', 'V', 'E', 'A', 'V', 'A', 'E', 'A', 'B', 'A', 'A', 'B', 'V', 'V', 'E', 'B', 'B', 'B', 'V', 'A', 'B', 'V', 'V', 'E', 'E', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'V', 'E', 'V', 'V', 'E', 'E', 'V', 'B', 'B', 'B', 'V', 'B', 'B', 'B', 'B', 'V', 'V', 'V', 'V', 'V', 'E', 'B', 'B', 'V', 'E', 'A', 'B', 'V', 'E', 'B', 'V', 'E', 'V', 'B', 'E', 'B', 'A', 'E', 'V', 'A', 'V', 'E', 'E', 'V', 'A', 'A', 'A', 'E', 'B', 'B', 'V', 'V', 'V', 'B', 'V', 'A', 'E', 'A', 'B', 'E', 'B', 'V', 'E', 'V', 'B', 'E', 'E', 'A', 'A', 'V', 'B', 'V', 'B', 'A', 'E', 'A', 'B', 'E', 'B', 'E', 'V', 'A', 'V', 'E', 'A', 'E', 'B', 'B', 'A', 'A', 'V', 'E', 'E', 'V', 'V', 'E', 'A', 'V', 'B', 'A', 'V', 'A', 'A', 'A', 'V', 'B', 'E', 'A', 'A', 'B', 'A', 'V', 'V', 'B', 'A', 'B', 'V', 'V', 'V', 'A', 'B', 'E', 'E', 'V', 'V', 'V', 'V', 'E', 'A', 'B', 'V', 'E', 'V', 'A', 'A', 'A', 'B', 'B', 'A', 'E', 'E', 'E', 'B', 'B', 'E', 'V', 'E', 'V', 'B', 'B', 'V', 'V', 'V', 'V', 'A', 'A', 'E', 'A', 'E', 'B', 'E', 'V', 'A', 'A', 'V', 'V', 'B', 'V', 'B', 'E', 'E', 'B', 'V', 'A', 'A', 'E', 'A', 'V', 'A', 'E', 'V', 'E', 'A', 'A', 'E', 'A', 'B', 'A', 'B', 'B', 'A', 'V', 'V', 'E', 'E', 'A', 'A', 'B', 'V', 'A', 'A', 'B', 'V', 'V', 'A', 'B', 'A', 'A', 'V', 'A', 'V', 'A', 'A', 'B', 'V', 'A', 'E', 'B', 'V', 'E', 'V', 'V', 'B', 'A', 'E', 'A', 'B', 'B', 'A', 'A', 'V', 'B', 'V', 'E', 'V', 'A', 'A', 'B', 'E', 'B', 'V', 'B', 'V', 'V', 'A', 'A', 'V', 'E', 'B', 'B', 'V', 'A', 'A', 'V', 'V', 'E', 'V', 'B', 'V', 'V', 'B', 'A', 'V', 'A', 'V', 'V', 'V', 'V', 'V', 'V', 'E', 'E', 'B', 'E', 'V', 'E', 'V', 'A', 'V', 'E', 'B', 'E', 'A', 'B', 'B', 'B', 'V', 'A', 'E', 'B', 'E', 'E', 'A', 'E', 'B', 'A', 'E', 'E', 'E', 'B', 'B', 'A', 'E', 'V', 'A', 'B', 'E', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'V', 'E', 'B', 'E', 'V', 'V', 'B', 'A', 'B', 'E', 'E', 'E', 'E', 'A', 'V', 'B', 'A', 'B', 'B', 'E', 'E', 'B', 'E', 'A', 'B', 'E', 'A', 'V', 'A', 'E', 'V', 'B', 'A', 'V', 'E', 'V', 'B', 'E', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'E', 'B', 'A', 'E', 'E', 'V', 'V', 'B', 'B', 'A', 'V', 'V', 'V', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'V', 'B', 'A', 'V', 'A', 'E', 'A', 'E', 'E', 'V', 'B', 'E', 'E', 'V', 'E', 'B', 'V', 'E', 'V', 'E', 'B', 'E', 'B', 'B', 'V', 'V', 'E', 'E', 'V', 'V', 'A', 'E', 'B', 'B', 'B', 'V', 'E', 'V', 'B', 'V', 'E', 'E', 'V', 'A', 'V', 'V', 'A', 'B', 'V', 'E', 'A', 'E', 'E', 'B', 'E', 'A', 'A', 'V', 'V', 'V', 'E', 'B', 'E', 'V', 'A', 'B', 'B', 'V', 'B', 'B', 'B', 'A', 'E', 'E', 'V', 'B', 'E', 'E', 'V', 'B', 'B', 'E', 'E', 'B', 'A', 'A', 'V', 'B', 'B', 'A', 'A', 'V', 'A', 'A', 'A', 'A', 'A', 'A', 'E', 'V', 'E', 'B', 'E', 'B', 'V', 'V', 'V', 'V', 'V', 'E', 'B', 'A', 'B', 'E', 'V', 'A', 'B', 'V', 'V', 'E', 'E', 'V', 'V', 'E', 'A', 'B', 'B', 'V', 'V', 'A', 'A', 'A', 'E', 'B', 'E', 'A', 'B', 'V', 'V', 'B', 'V', 'A', 'A', 'E', 'B', 'B', 'V', 'V', 'E', 'E', 'V', 'B', 'V', 'V', 'V', 'E', 'B', 'A', 'A', 'E', 'V', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'V', 'E', 'E', 'B', 'A', 'V', 'V', 'A', 'A', 'B', 'B', 'V', 'E', 'V', 'V', 'A', 'A', 'A', 'B', 'E', 'V', 'A', 'B', 'B', 'A', 'E', 'B', 'B', 'E', 'V', 'A', 'B', 'V', 'V', 'A', 'B', 'E', 'V', 'B', 'A', 'B', 'A', 'E', 'A', 'E', 'A', 'V', 'B', 'A', 'V', 'B', 'E', 'E', 'B', 'B', 'E', 'A', 'B', 'V', 'A', 'A', 'E', 'A', 'E', 'E', 'E', 'E', 'V', 'E', 'V', 'V', 'V', 'E', 'E', 'V', 'B', 'V', 'E', 'A', 'V', 'B', 'V', 'B', 'B', 'B', 'V', 'B', 'V', 'E', 'V', 'B', 'V', 'B', 'B', 'B', 'E', 'E', 'V', 'E', 'E', 'E', 'V', 'A', 'V', 'A', 'E', 'B', 'B', 'A', 'B', 'E', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'B', 'V', 'V', 'B', 'E', 'V', 'V', 'B', 'V', 'V', 'E', 'E', 'A', 'E', 'A', 'E', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'E', 'B', 'A', 'A', 'E', 'A', 'V', 'V', 'B', 'A', 'B', 'V', 'A', 'A', 'A', 'A', 'E', 'A', 'E', 'A', 'E', 'E', 'V', 'V', 'E', 'A', 'A', 'A', 'B', 'B', 'E', 'E', 'A', 'V', 'E', 'E', 'V', 'A', 'V', 'E', 'V', 'A', 'A', 'E', 'A', 'B', 'V', 'E', 'A', 'V', 'A', 'A', 'B', 'B', 'V', 'B', 'V', 'A', 'V', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'A', 'A', 'E', 'E', 'E', 'A', 'A', 'E', 'B', 'V', 'A', 'E', 'A', 'B', 'E', 'V', 'B', 'V', 'E', 'V', 'E', 'A', 'A', 'A', 'E', 'A', 'E', 'V', 'E', 'A', 'V', 'E', 'B', 'E', 'E', 'E', 'V', 'V', 'A', 'A', 'E', 'A', 'B', 'E', 'V', 'V', 'V', 'V', 'V', 'V', 'A', 'B', 'A', 'E', 'V', 'E', 'B', 'V', 'E', 'B', 'V', 'E', 'B', 'E', 'A', 'B', 'A', 'B', 'V', 'A', 'A', 'V', 'V', 'A', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'A', 'V', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'V', 'E', 'A', 'B', 'B', 'V', 'V', 'E', 'E', 'V', 'E', 'E', 'A', 'E', 'V', 'B', 'A', 'B', 'E', 'A', 'V', 'V', 'V', 'B', 'V', 'A', 'E', 'V', 'E', 'E', 'E', 'V', 'A', 'A', 'B', 'E', 'E', 'B', 'V', 'V', 'B', 'V', 'V', 'V', 'B', 'A', 'E', 'A', 'V', 'A', 'E', 'B', 'A', 'E', 'B', 'V', 'B', 'E', 'E', 'B', 'A', 'E', 'E', 'V', 'E', 'E', 'A', 'V', 'B', 'B', 'E', 'V', 'A', 'B', 'V', 'V', 'E', 'A', 'B', 'B', 'B', 'E', 'V', 'B', 'A', 'E', 'B', 'B', 'E', 'A', 'B', 'V', 'A', 'A', 'E', 'E', 'B', 'B', 'V', 'B', 'A', 'A', 'E', 'B', 'B', 'B', 'A', 'E', 'A', 'A', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'V', 'B', 'A', 'V', 'A', 'E', 'B', 'E', 'E', 'A', 'V', 'A', 'A', 'E', 'A', 'B', 'B', 'E', 'B', 'B', 'B', 'E', 'V', 'E', 'E', 'V', 'A', 'V', 'B', 'B', 'V', 'V', 'E', 'V', 'E', 'E', 'A', 'A', 'E', 'V', 'B', 'B', 'E', 'A', 'V', 'B', 'A', 'V', 'E', 'E', 'E', 'A', 'A', 'A', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'A', 'A', 'B', 'V', 'E', 'A', 'V', 'V', 'E', 'V', 'E', 'E', 'A', 'B', 'E', 'A', 'V', 'A', 'A', 'E', 'E', 'V', 'E', 'A', 'V', 'A', 'V', 'V', 'A', 'B', 'B', 'A', 'B', 'E', 'A', 'A', 'A', 'E', 'A', 'B', 'V', 'V', 'A', 'E', 'E', 'E', 'B', 'V', 'E', 'B', 'A', 'E', 'E', 'B', 'E', 'B', 'V', 'B', 'V', 'A', 'A', 'V', 'A', 'A', 'V', 'V', 'V', 'A', 'A', 'V', 'V', 'E', 'A', 'E', 'V', 'V', 'A', 'E', 'V', 'V', 'B', 'B', 'A', 'A', 'E', 'E', 'B', 'A', 'E', 'B', 'V', 'E', 'B', 'V', 'A', 'V', 'B', 'V', 'V', 'E', 'V', 'A', 'A', 'E', 'E', 'E', 'B', 'V', 'E', 'V', 'E', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'V', 'B', 'E', 'B', 'B', 'V', 'V', 'A', 'E', 'V', 'B', 'E', 'A', 'B', 'A', 'B', 'E', 'B', 'V', 'V', 'V', 'A', 'B', 'B', 'V', 'E', 'E', 'B', 'B', 'A', 'B', 'A', 'V', 'A', 'V', 'A', 'A', 'A', 'A', 'E', 'B', 'V', 'A', 'V', 'E', 'E', 'E', 'V', 'A', 'V', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'A', 'A', 'B', 'B', 'A', 'E', 'B', 'E', 'B', 'V', 'E', 'V', 'B', 'V', 'E', 'B', 'V', 'A', 'A', 'E', 'E', 'A', 'B', 'A', 'E', 'E', 'V', 'V', 'B', 'A', 'V', 'A', 'B', 'E', 'E', 'A', 'B', 'B', 'E', 'B', 'V', 'A', 'A', 'V', 'B', 'V', 'A', 'A', 'B', 'V', 'A', 'B', 'B', 'B', 'A', 'E', 'E', 'E', 'V', 'A', 'V', 'V', 'V', 'V', 'V', 'V', 'B', 'V', 'V', 'A', 'E', 'E', 'B', 'E', 'A', 'V', 'B', 'E', 'A', 'E', 'V', 'E', 'A', 'V', 'V', 'B', 'B', 'V', 'E', 'E', 'A', 'E', 'B', 'V', 'B', 'V', 'E', 'V', 'V', 'E', 'A', 'A', 'V', 'A', 'A', 'E', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'V', 'A', 'A', 'A', 'A', 'E', 'V', 'E', 'V', 'A', 'E', 'A', 'B', 'E', 'E', 'V', 'E', 'E', 'V', 'B', 'B', 'E', 'B', 'A', 'E', 'B', 'B', 'A', 'E', 'E', 'B', 'B', 'E', 'V', 'V', 'A', 'V', 'B', 'A', 'A', 'A', 'B', 'V', 'B', 'V', 'E', 'E', 'A', 'V', 'V', 'V', 'A', 'E', 'E', 'E', 'V', 'A', 'V', 'E', 'A', 'B', 'A', 'B', 'B', 'E', 'V', 'V', 'B', 'V', 'V', 'B', 'E', 'B', 'B', 'A', 'E', 'E', 'A', 'A', 'E', 'V', 'V', 'A', 'B', 'V', 'V', 'A', 'V', 'B', 'E', 'B', 'B', 'B', 'A', 'E', 'B', 'E', 'V', 'E', 'B', 'B', 'B', 'V', 'E', 'A', 'B', 'A', 'E', 'B', 'E', 'B', 'V', 'E', 'B', 'E', 'B', 'V', 'B', 'A', 'V', 'E', 'V', 'A', 'V', 'E', 'V', 'A', 'V', 'V', 'A', 'V', 'A', 'E', 'V', 'E', 'V', 'A', 'V', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'E', 'V', 'V', 'B', 'E', 'E', 'B', 'B', 'V', 'V', 'V', 'V', 'E', 'E', 'V', 'E', 'E', 'E', 'V', 'A', 'V', 'B', 'V', 'E', 'A', 'B', 'B', 'A', 'V', 'V', 'V', 'E', 'V', 'E', 'V', 'E', 'A', 'A', 'V', 'E', 'V', 'E', 'V', 'A', 'A', 'B', 'E', 'B', 'B', 'V', 'V', 'V', 'E', 'E', 'A', 'A', 'E', 'V', 'A', 'E', 'V', 'V', 'B', 'B', 'V', 'E', 'E', 'B', 'E', 'V', 'V', 'E', 'E', 'V', 'B', 'E', 'B', 'V', 'B', 'A', 'A', 'A', 'B', 'E', 'E', 'V', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'V', 'B', 'V', 'B', 'V', 'V', 'B', 'A', 'B', 'B', 'A', 'E', 'B', 'E', 'V', 'V', 'V', 'B', 'V', 'E', 'B', 'E', 'E', 'B', 'A', 'V', 'A', 'V', 'E', 'A', 'E', 'A', 'B', 'V', 'A', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'V', 'B', 'V', 'A', 'A', 'E', 'E', 'E', 'A', 'E', 'A', 'E', 'A', 'E', 'A', 'A', 'V', 'E', 'A', 'A', 'B', 'V', 'E', 'A', 'E', 'E', 'A', 'B', 'V', 'E', 'V', 'V', 'A', 'V', 'E', 'B', 'V', 'E', 'B', 'E', 'V', 'V', 'V', 'A', 'V', 'V', 'A', 'A', 'V', 'E', 'E', 'E', 'A', 'V', 'E', 'A', 'V', 'E', 'E', 'B', 'B', 'A', 'A', 'E', 'A', 'V', 'B', 'E', 'E', 'B', 'E', 'V', 'E', 'V', 'V', 'B', 'A', 'A', 'A', 'E', 'A', 'A', 'E', 'A', 'V', 'B', 'E', 'V', 'E', 'V', 'E', 'B', 'E', 'A', 'E', 'E', 'V', 'E', 'B', 'A', 'B', 'B', 'A', 'A', 'E', 'A', 'V', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'V', 'B', 'B', 'B', 'V', 'E', 'V', 'E', 'B', 'E', 'B', 'V', 'V', 'V', 'A', 'V', 'A', 'B', 'A', 'V', 'A', 'V', 'B', 'A', 'A', 'E', 'A', 'B', 'V', 'E', 'V', 'E', 'V', 'V', 'E', 'E', 'E', 'E', 'E', 'A', 'B', 'A', 'V', 'A', 'E', 'A', 'E', 'E', 'E', 'A', 'E', 'V', 'A', 'E', 'V', 'B', 'V', 'B', 'E', 'E', 'A', 'E', 'V', 'A', 'A', 'A', 'V', 'A', 'A', 'E', 'B', 'E', 'E', 'B', 'E', 'A', 'E', 'E', 'A', 'B', 'E', 'E', 'E', 'V', 'E', 'B', 'E', 'E', 'V', 'E', 'V', 'V', 'A', 'V', 'A', 'B', 'V', 'B', 'E', 'V', 'V', 'B', 'V', 'B', 'A', 'V', 'E', 'E', 'E', 'E', 'V', 'V', 'E', 'V', 'B', 'B', 'V', 'A', 'E', 'E', 'E', 'B', 'A', 'B', 'B', 'V', 'A', 'A', 'A', 'E', 'B', 'V', 'V', 'B', 'B', 'V', 'V', 'V', 'A', 'E', 'B', 'V', 'B', 'E', 'B', 'E', 'A', 'E', 'B', 'B', 'A', 'E', 'V', 'B', 'B', 'E', 'B', 'V', 'E', 'A', 'B', 'E', 'V', 'V', 'B', 'V', 'V', 'A', 'A', 'E', 'V', 'V', 'E', 'E', 'V', 'V', 'B', 'A', 'B', 'B', 'E', 'A', 'A', 'V', 'E', 'A', 'A', 'E', 'B', 'A', 'A', 'B', 'A', 'V', 'E', 'B', 'A', 'A', 'B', 'E', 'A', 'E', 'A', 'B', 'B', 'A', 'V', 'V', 'V', 'E', 'E', 'A', 'E', 'V', 'V', 'A', 'E', 'B', 'E', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'V', 'E', 'E', 'E', 'A', 'E', 'B', 'A', 'E', 'A', 'B', 'A', 'V', 'A', 'E', 'E', 'B', 'V', 'B', 'B', 'A', 'E', 'V', 'B', 'B', 'E', 'B', 'V', 'B', 'E', 'V', 'A', 'B', 'E', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'E', 'B', 'E', 'A', 'V', 'B', 'B', 'E', 'V', 'V', 'A', 'V', 'A', 'V', 'V', 'B', 'B', 'B', 'A', 'E', 'E', 'E', 'B', 'A', 'E', 'B', 'E', 'V', 'B', 'A', 'E', 'B', 'A', 'E', 'B', 'A', 'B', 'B', 'E', 'A', 'E', 'V', 'E', 'E', 'A', 'A', 'V', 'B', 'V', 'A', 'V', 'V', 'E', 'A', 'V', 'A', 'E', 'B', 'E', 'E', 'V', 'A', 'B', 'B', 'V', 'A', 'E', 'V', 'A', 'V', 'B', 'V', 'A', 'B', 'A', 'E', 'A', 'B', 'A', 'B', 'A', 'B', 'E', 'V', 'B', 'E', 'V', 'E', 'B', 'E', 'E', 'A', 'E', 'B', 'E', 'E', 'B', 'B', 'A', 'E', 'B', 'E', 'B', 'A', 'B', 'E', 'A', 'B', 'A', 'V', 'V', 'E', 'B', 'A', 'V', 'B', 'E', 'E', 'A', 'B', 'A', 'A', 'E', 'B', 'E', 'E', 'B', 'A', 'B', 'B', 'B', 'E', 'V', 'E', 'E', 'E', 'B', 'B', 'A', 'E', 'V', 'V', 'A', 'V', 'B', 'A', 'V', 'B', 'B', 'A', 'V', 'E', 'V', 'E', 'V', 'E', 'E', 'E', 'V', 'B', 'B', 'A', 'E', 'B', 'A', 'A', 'V', 'V', 'B', 'E', 'V', 'V', 'E', 'B', 'V', 'V', 'B', 'E', 'E', 'V', 'A', 'A', 'E', 'V', 'V', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'V', 'E', 'V', 'V', 'V', 'V', 'A', 'A', 'B', 'E', 'V', 'E', 'V', 'B', 'E', 'E', 'B', 'B', 'E', 'V', 'B', 'V', 'E', 'A', 'B', 'V', 'A', 'V', 'A', 'V', 'V', 'E', 'B', 'V', 'B', 'E', 'V', 'B', 'A', 'B', 'A', 'A', 'A', 'E', 'A', 'V', 'V', 'B', 'A', 'A', 'V', 'V', 'A', 'A', 'E', 'A', 'B', 'V', 'V', 'B', 'A', 'B', 'V', 'V', 'V', 'V', 'B', 'E', 'A', 'B', 'V', 'E', 'V', 'A', 'E', 'V', 'V', 'B', 'V', 'V', 'B', 'B', 'V', 'V', 'E', 'V', 'E', 'A', 'B', 'V', 'V', 'V', 'E', 'E', 'V', 'B', 'B', 'B', 'E', 'E', 'B', 'A', 'V', 'V', 'E', 'V', 'E', 'B', 'A']\n",
      "Training acc: 0.9868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler   \n",
    "\n",
    "df = pd.read_csv(\"trg.csv\")\n",
    "df = df.rename({'class':'target'},axis=1)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "df_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "df_resampled = df_resampled.sample(frac=1, random_state=42)\n",
    "\n",
    "training, testing = train_test_split(df_resampled, test_size=0.3, random_state=42)\n",
    "vocab, prior_probability, conditional_probability_dict,conditional_probability_df = Improved_train(training)\n",
    "prediction_row = Improved_Naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df, testing)\n",
    "print(prediction_row)\n",
    "validation_acc = (testing['target'] == prediction_row).mean()\n",
    "print(f\"Training acc: {validation_acc:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Improved Naive Bayes impletation yields a validation accuracy of 94.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting....\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"trg.csv\")\n",
    "df = df.rename({'class':'target'},axis=1)\n",
    "test_df = pd.read_csv(\"tst.csv\")\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "df_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "df_resampled = df_resampled.sample(frac=1, random_state=42)\n",
    "vocab, prior_probability, conditional_probability_dict,conditional_probability_df = Improved_train(df_resampled)\n",
    "\n",
    "prediction_row = Improved_Naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df, test_df)\n",
    "\n",
    "with open(\"tst.csv\",\"rt\") as source:\n",
    "    rdr = csv.reader(source)\n",
    "    c = -1\n",
    "    with open(\"raar518.csv\",\"wt\", newline='') as result:\n",
    "        wtr = csv.writer( result )\n",
    "        for r in rdr:\n",
    "            if (c==-1): wtr.writerow( (r[0], \"class\") ) #add csv header\n",
    "            else: wtr.writerow( (r[0], prediction_row[c]) )\n",
    "            c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
