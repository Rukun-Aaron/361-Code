{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from decimal import Decimal\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(text, n):\n",
    "\n",
    "    # print(text)\n",
    "    words = text\n",
    "\n",
    "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "# def get_ngram(df):\n",
    "\n",
    "#     for line in df['abstract']:\n",
    "#         words = [word for word in line.split() if word not in stopWords]\n",
    "#         bigram = ngram(words, 2)\n",
    "        \n",
    "def get_vocab(df):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    vocab = {word for para in df['abstract'] for word in para.split() if word not in stopWords}\n",
    "    # for line in df['abstract']:\n",
    "    #     words = [word for word in line.split() if word not in stopWords]\n",
    "    #     bigram = get_ngram(words, 2)\n",
    "    #     vocab.update(bigram)\n",
    "        # if 'regulatory control' in bigram:\n",
    "        #     print(bigram)\n",
    "\n",
    "\n",
    "    # vocabulary = set() \n",
    "    # vocabulary = {word for line in df['abstract'] for word in line.split() if word not in stopWords}\n",
    "    # print(len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def get_prior_probability(df):\n",
    "    prior_probability = df['target'].value_counts(normalize=True).to_dict()\n",
    "    for key, value in prior_probability.items():\n",
    "        prior_probability[key] = Decimal(str(value))\n",
    "    return prior_probability \n",
    "\n",
    "def get_conditional_probability(df,vocab):\n",
    "    count = 0\n",
    "    conditional_probability = dict()\n",
    "    # ngram =[]\n",
    "    for row in df.itertuples(index = 0): \n",
    "        count+=1\n",
    "        # print(count)\n",
    "        line = row.abstract\n",
    "        target = row.target\n",
    "        # print(target)\n",
    "        words = [word for word in line.split() if word not in stopWords]\n",
    "        \n",
    "        # ngram_1 =get_ngram(words,2)\n",
    "\n",
    "        # print(ngram_1)\n",
    "        \n",
    "        # for word in words + ngram_1:\n",
    "        for word in words :\n",
    "            if word == \"regulatory control\":\n",
    "                print(word)\n",
    "            \n",
    "            if word in conditional_probability.keys():\n",
    "                if target in conditional_probability[word].keys():\n",
    "                    conditional_probability[word][target] += 1\n",
    "                else:\n",
    "                    conditional_probability[word][target] = 1\n",
    "            else:\n",
    "                conditional_probability[word] = {\"B\": Decimal(0), \"E\":Decimal(0), \"A\":Decimal(0), \"V\":Decimal(0)}\n",
    "                conditional_probability[word][target] += 1\n",
    "    return conditional_probability\n",
    "\n",
    "def get_conditional_probability_df(conditional_probability):\n",
    "\n",
    "    data = []\n",
    "    for word, targets in conditional_probability.items():\n",
    "        \n",
    "        row = {'word': word}\n",
    "        \n",
    "        for i, (target, count) in enumerate(targets.items()):\n",
    "            row [target] = count\n",
    "        # append the row to the data list\n",
    "        data.append(row)\n",
    "    word_orrurance_table = pd.DataFrame(data)\n",
    "    word_orrurance_table.fillna(0, inplace=True)\n",
    "    return word_orrurance_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train(df):\n",
    "    vocab = get_vocab(df)\n",
    "    prior_probability = get_prior_probability(df)\n",
    "    conditional_probability_dict = get_conditional_probability(df,vocab)\n",
    "    conditional_probability_df = get_conditional_probability_df(conditional_probability_dict)\n",
    "    return vocab, prior_probability, conditional_probability_dict,conditional_probability_df\n",
    "\n",
    "def naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df,testing):\n",
    "    predictions = []\n",
    "    row_nm = 0\n",
    "    for row in testing.itertuples(index = 0):\n",
    "        row_nm += 1\n",
    "        # print(row_nm)\n",
    "        line = row.abstract\n",
    "        prediction = predict(vocab,prior_probability,conditional_probability_dict,conditional_probability_df,line)\n",
    "        # print(prediction, row.target)\n",
    "        predictions.append(prediction)\n",
    "    # testing['prediction'] = predictions\n",
    "    return predictions\n",
    "\n",
    "def predict(vocab,prior_probability,conditional_probability_dict,conditional_probability_df,line):\n",
    "    words = line.split()\n",
    "    # ngrams = get_ngram(words, 2)\n",
    "    # print(words)\n",
    "    prob_B = prior_probability['B'].ln()\n",
    "    prob_E = prior_probability['E'].ln()\n",
    "    prob_A = prior_probability['A'].ln()\n",
    "    prob_V = prior_probability['V'].ln()\n",
    "\n",
    "    total_B = conditional_probability_df['B'].sum()\n",
    "    total_E = conditional_probability_df['E'].sum()\n",
    "    total_A = conditional_probability_df['A'].sum()\n",
    "    total_V = conditional_probability_df['V'].sum()\n",
    "    # for word in words + ngrams:\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            prob_B += (conditional_probability_dict[word]['B'] + 1).ln() - (Decimal(total_B + len(vocab))).ln()\n",
    "            prob_A += (conditional_probability_dict[word]['A'] + 1).ln() - (Decimal(total_A + len(vocab))).ln()\n",
    "            prob_V += (conditional_probability_dict[word]['V'] + 1).ln() - (Decimal(total_V + len(vocab))).ln()\n",
    "            prob_E += (conditional_probability_dict[word]['E'] + 1).ln() - (Decimal(total_E + len(vocab))).ln()\n",
    "    # print(prob_B,prob_E,prob_A,prob_V)\n",
    "    if prob_B > prob_E and prob_B > prob_A and prob_B > prob_V:\n",
    "        return 'B'\n",
    "    elif prob_E > prob_B and prob_E > prob_A and prob_E > prob_V:\n",
    "        return 'E'\n",
    "    elif prob_A > prob_B and prob_A > prob_E and prob_A > prob_V:\n",
    "        return 'A'\n",
    "    elif prob_V > prob_B and prob_V > prob_E and prob_V > prob_A:\n",
    "        return 'V'\n",
    "    else:\n",
    "        return 'E'\n",
    "\n",
    "# vocab, prior_probability, conditional_probability_dict,conditional_probability_df = train(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def split_train_test(df):\n",
    "#     df = df.sample(frac=1, random_state=32)\n",
    "\n",
    "#     count_rows = len(df.index)\n",
    "#     count_train_set = round(count_rows * 0.7)\n",
    "\n",
    "#     train_set_df = df.iloc[:count_train_set]\n",
    "#     validation_set_df = df.iloc[count_train_set:]\n",
    "\n",
    "#     return train_set_df,validation_set_df\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_csv(\"trg.csv\")\n",
    "# df = df.rename({'class':'target'},axis=1)\n",
    "# training, testing = train_test_split(df, test_size=0.3, random_state=42)\n",
    "# # training, testing = split_train_test(df)\n",
    "# vocab, prior_probability, conditional_probability_dict,conditional_probability_df = train(training)\n",
    "# prediction_row = naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df, testing)\n",
    "# print(prediction_row)\n",
    "# # print(testing['target'])\n",
    "# validation_acc = (testing['target'] == prediction_row).mean()\n",
    "# print(f\"Training acc: {validation_acc:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trg.csv\")\n",
    "df = df.rename({'class':'target'},axis=1)\n",
    "testing = pd.read_csv(\"tst.csv\")\n",
    "test_df = pd.read_csv(\"tst.csv\")\n",
    "vocab, prior_probability, conditional_probability_dict,conditional_probability_df = train(df)\n",
    "\n",
    "prediction_row = naive_Bayes(vocab, prior_probability, conditional_probability_dict,conditional_probability_df, test_df)\n",
    "\n",
    "with open(\"tst.csv\",\"rt\") as source:\n",
    "    rdr = csv.reader(source)\n",
    "    c = -1\n",
    "    with open(\"raar518.csv\",\"wt\", newline='') as result:\n",
    "        wtr = csv.writer( result )\n",
    "        for r in rdr:\n",
    "            if (c==-1): wtr.writerow( (r[0], \"class\") ) #add csv header\n",
    "            else: wtr.writerow( (r[0], prediction_row[c]) )\n",
    "            c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
